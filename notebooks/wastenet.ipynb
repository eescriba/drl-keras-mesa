{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.7 64-bit ('venv': venv)",
      "metadata": {
        "interpreter": {
          "hash": "915e8579bc6bc5b7e6bdee387f64d79c54c721fc11f17b28b0b204a29bde1622"
        }
      }
    },
    "colab": {
      "name": "rllib.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO9547wOqrGb"
      },
      "source": [
        "# WasteNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWzkFkIFqz4C"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pLSISOhRLDK"
      },
      "source": [
        "# Colab\n",
        "!rm -r smart-cities-drl\n",
        "!git clone https://github.com/eescriba/smart-cities-drl\n",
        "!cd smart-cities-drl/ && pip install -e .\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,'./smart-cities-drl/src/')\n",
        "\n",
        "# Local\n",
        "# !pip install -e ..\n",
        "# import sys\n",
        "# sys.path.insert(0,'../src/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qv2IiHGYRLDQ"
      },
      "source": [
        "import json\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "import gym\n",
        "import ray\n",
        "import ray.rllib.agents.ppo as ppo\n",
        "from ray.tune import run, sample_from\n",
        "from ray.tune.logger import pretty_print\n",
        "from ray.tune.registry import register_env\n",
        "from ray.tune.schedulers import PopulationBasedTraining\n",
        "import tensorflow as tf\n",
        "\n",
        "from wastenet.env import WasteNetEnv"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  0\n"
          ]
        }
      ],
      "source": [
        "register_env(\"WasteNet-v0\", lambda config: WasteNetEnv(config))\n",
        "num_gpus = len(tf.config.list_physical_devices('GPU'))\n",
        "print(\"Num GPUs Available: \", num_gpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-05-23 18:01:13,196\tINFO services.py:1267 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'node_ip_address': '192.168.100.180',\n",
              " 'raylet_ip_address': '192.168.100.180',\n",
              " 'redis_address': '192.168.100.180:6379',\n",
              " 'object_store_address': '/tmp/ray/session_2021-05-23_18-01-10_132184_46580/sockets/plasma_store',\n",
              " 'raylet_socket_name': '/tmp/ray/session_2021-05-23_18-01-10_132184_46580/sockets/raylet',\n",
              " 'webui_url': '127.0.0.1:8265',\n",
              " 'session_dir': '/tmp/ray/session_2021-05-23_18-01-10_132184_46580',\n",
              " 'metrics_export_port': 47135,\n",
              " 'node_id': '54ad67fc224d4ee2d2b27f9b809a3100bbd8caf4ff2177a84199c3f2'}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "ray.shutdown()\n",
        "ray.init(ignore_reinit_error=True, num_gpus=num_gpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hrqtufqq24H"
      },
      "source": [
        "## Proximal Policy Optimization (PPO)"
      ]
    },
    {
      "source": [
        "### Population Based Training (PBT)"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Postprocess the perturbed config to ensure it's still valid used if PBT.\n",
        "def explore(config):\n",
        "    # ensure we collect enough timesteps to do sgd\n",
        "    if config[\"train_batch_size\"] < config[\"sgd_minibatch_size\"] * 2:\n",
        "        config[\"train_batch_size\"] = config[\"sgd_minibatch_size\"] * 2\n",
        "    # ensure we run at least one sgd iter\n",
        "    if config[\"num_sgd_iter\"] < 1:\n",
        "        config[\"num_sgd_iter\"] = 1\n",
        "    return config\n",
        "\n",
        "pbt = PopulationBasedTraining(\n",
        "    time_attr=\"time_total_s\",\n",
        "    perturbation_interval=120,\n",
        "    resample_probability=0.25,\n",
        "    metric=\"episode_reward_mean\",\n",
        "    mode=\"max\",\n",
        "    # Specifies the mutations of these hyperparams\n",
        "    hyperparam_mutations={\n",
        "        \"lambda\": lambda: random.uniform(0.7, 1.0),\n",
        "        \"clip_param\": lambda: random.uniform(0.01, 0.5),\n",
        "        \"lr\": [1e-3, 5e-4, 1e-4, 5e-5, 1e-5],\n",
        "        \"num_sgd_iter\": lambda: random.randint(1, 30),\n",
        "        \"sgd_minibatch_size\": lambda: random.randint(128, 16384),\n",
        "        \"train_batch_size\": lambda: random.randint(2000, 160000),\n",
        "    },\n",
        "    custom_explore_fn=explore)"
      ]
    },
    {
      "source": [
        "### Tune hyperparameters"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "analysis = run(\n",
        "        'PPO',\n",
        "        name=\"wastenet_ppo\",\n",
        "        local_dir=\"./ray_results\",\n",
        "        resume=True,\n",
        "        scheduler=pbt,\n",
        "        num_samples=8,\n",
        "        config={\n",
        "            \"env\": WasteNetEnv,\n",
        "            \"seed\": 123,\n",
        "            \"num_gpus\": num_gpus,\n",
        "            \"num_workers\": 1,\n",
        "            \"observation_filter\": \"MeanStdFilter\",\n",
        "            \"model\": {\n",
        "                # \"fcnet_hiddens\": [\n",
        "                #     32,\n",
        "                #     32\n",
        "                # ],\n",
        "                \"free_log_std\": True\n",
        "            },\n",
        "            \"num_sgd_iter\": 10,\n",
        "            \"sgd_minibatch_size\": 128,\n",
        "            \"lambda\": sample_from(lambda spec: random.uniform(0.9, 1.0)),\n",
        "            \"clip_param\": sample_from(lambda spec: random.uniform(0.1, 0.5)),\n",
        "            \"lr\": sample_from(lambda spec: random.uniform(1e-3, 1e-5)),\n",
        "            \"train_batch_size\": sample_from(\n",
        "                lambda spec: random.randint(1000, 60000))\n",
        "        })\n",
        "\n",
        "best_config=analysis.best_config\n",
        "print(\"best hyperparameters: \", best_config)"
      ]
    },
    {
      "source": [
        "### Training"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsi17r9uSE-2"
      },
      "source": [
        "best_config = {\n",
        "    \"observation_filter\": \"MeanStdFilter\",\n",
        "    \"model\": {\"free_log_std\": True},\n",
        "    \"num_sgd_iter\": 10,\n",
        "    \"sgd_minibatch_size\": 128,\n",
        "    \"lambda\": 0.731396,\n",
        "    \"clip_param\": 0.317651,\n",
        "    \"lr\": 5e-05,\n",
        "    \"train_batch_size\": 18812,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "config = best_config\n",
        "config[\"num_gpus\"] = num_gpus\n",
        "\n",
        "agent = ppo.PPOTrainer(config, env=WasteNetEnv)\n",
        "\n",
        "N_ITER = 1000\n",
        "\n",
        "for n in range(N_ITER):\n",
        "    result = agent.train()\n",
        "    print(pretty_print(result))\n",
        "    if i % 10 == 0:\n",
        "        checkpoint = trainer.save()\n",
        "        print(\"checkpoint saved at\", checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE5CrWozRLDS"
      },
      "source": [
        "policy = agent.get_policy()\n",
        "model = policy.model\n",
        "print(model.base_model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_P4Tl-1zsU2h"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2dIN-hNsXOM"
      },
      "source": [
        "!rllib rollout \\\n",
        "  --run PPO  \\\n",
        "  --env WasteNet-v0  \\\n",
        "  --steps 1000 \\\n",
        "  --config best_config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "obs = env.reset()\n",
        "done = False\n",
        "episode_reward = 0\n",
        "sum_reward = 0\n",
        "n_step = 1000\n",
        "for step in range(n_step):\n",
        "    action = agent.compute_action(obs)\n",
        "    print(action)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    print(state, reward, done, info)\n",
        "    sum_reward += reward\n",
        "    if done:\n",
        "        print(\"cumulative reward\", sum_reward)\n",
        "        state = env.reset()\n",
        "        sum_reward = 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qdEKhi5sWoX"
      },
      "source": [
        "%load_ext tensorboard \n",
        "%tensorboard --logdir=\"/content/ray_results/wastenet_ppo\""
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
            "TensorBoard 2.4.1 at http://localhost:6006/ (Press CTRL+C to quit)\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}